{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Assignment_8B_LR_SVM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ArWK463kbhL",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a050620e-d019-4f75-a860-c8333886f3b4"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly\n",
        "import plotly.figure_factory as ff\n",
        "import plotly.graph_objs as go\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "init_notebook_mode(connected=True)\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a8e495c3-bec4-4fca-b88f-495ba598c8d5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a8e495c3-bec4-4fca-b88f-495ba598c8d5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving task_b.csv to task_b.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mldzJdakbhS"
      },
      "source": [
        "data = pd.read_csv('task_b.csv')\n",
        "data=data.iloc[:,1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "rsCrC2wckbhV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b1d34fca-6c68-4a77-a723-21f30b22b8e5"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-195.871045</td>\n",
              "      <td>-14843.084171</td>\n",
              "      <td>5.532140</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1217.183964</td>\n",
              "      <td>-4068.124621</td>\n",
              "      <td>4.416082</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.138451</td>\n",
              "      <td>4413.412028</td>\n",
              "      <td>0.425317</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>363.824242</td>\n",
              "      <td>15474.760647</td>\n",
              "      <td>1.094119</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-768.812047</td>\n",
              "      <td>-7963.932192</td>\n",
              "      <td>1.870536</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            f1            f2        f3    y\n",
              "0  -195.871045 -14843.084171  5.532140  1.0\n",
              "1 -1217.183964  -4068.124621  4.416082  1.0\n",
              "2     9.138451   4413.412028  0.425317  0.0\n",
              "3   363.824242  15474.760647  1.094119  0.0\n",
              "4  -768.812047  -7963.932192  1.870536  0.0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI18joJ_kbhZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "006dc9ea-e096-4a70-9e07-047936fcadde"
      },
      "source": [
        "data.corr()['y']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "f1    0.067172\n",
              "f2   -0.017944\n",
              "f3    0.839060\n",
              "y     1.000000\n",
              "Name: y, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "u40oCVMikbhc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f69636a6-51c9-47c4-f83a-70fd12ba3646"
      },
      "source": [
        "data.std()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "f1      488.195035\n",
              "f2    10403.417325\n",
              "f3        2.926662\n",
              "y         0.501255\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQIbNaHskbhe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b4e81f4-c60a-4693-9a2f-5506339a5369"
      },
      "source": [
        "X=data[['f1','f2','f3']].values\n",
        "Y=data['y'].values\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200, 3)\n",
            "(200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUxp9-qEkbhh"
      },
      "source": [
        "# What if our features are with different variance \n",
        "\n",
        "<pre>\n",
        "* <b>As part of this task you will observe how linear models work in case of data having feautres with different variance</b>\n",
        "* <b>from the output of the above cells you can observe that var(F2)>>var(F1)>>Var(F3)</b>\n",
        "\n",
        "> <b>Task1</b>:\n",
        "    1. Apply Logistic regression(SGDClassifier with logloss) on 'data' and check the feature importance\n",
        "    2. Apply SVM(SGDClassifier with hinge) on 'data' and check the feature importance\n",
        "\n",
        "> <b>Task2</b>:\n",
        "    1. Apply Logistic regression(SGDClassifier with logloss) on 'data' after standardization \n",
        "       i.e standardization(data, column wise): (column-mean(column))/std(column) and check the feature importance\n",
        "    2. Apply SVM(SGDClassifier with hinge) on 'data' after standardization \n",
        "       i.e standardization(data, column wise): (column-mean(column))/std(column) and check the feature importance\n",
        "\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWaXSttOjklJ",
        "outputId": "c53d1c20-9bb5-4de3-bc7f-b295a27a0f28"
      },
      "source": [
        "from sklearn import linear_model\n",
        "clf_lr_nstd = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
        "clf_lr_nstd.fit(X,Y)\n",
        "print(clf_lr_nstd.coef_)\n",
        "print(clf_lr_nstd.intercept_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 1.08, NNZs: 3, Bias: -0.001751, T: 200, Avg. loss: 2516.147588\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.61, NNZs: 3, Bias: -0.001551, T: 400, Avg. loss: 2621.694380\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.35, NNZs: 3, Bias: -0.001850, T: 600, Avg. loss: 3285.222158\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.64, NNZs: 3, Bias: -0.003527, T: 800, Avg. loss: 3142.216822\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.48, NNZs: 3, Bias: -0.004027, T: 1000, Avg. loss: 3009.886714\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1.40, NNZs: 3, Bias: -0.003523, T: 1200, Avg. loss: 3032.001946\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 6 epochs took 0.00 seconds\n",
            "[[ 0.37170471 -1.34463853  0.12669033]]\n",
            "[-0.00352309]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVGS5LDzmk6C",
        "outputId": "fc0059d1-92eb-4acd-d6b6-7a253873508a"
      },
      "source": [
        "from sklearn import linear_model\n",
        "clf_SVM_Nstd = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='hinge', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
        "clf_SVM_Nstd.fit(X,Y)\n",
        "print(clf_SVM_Nstd.coef_)\n",
        "print(clf_SVM_Nstd.intercept_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 0.61, NNZs: 3, Bias: -0.001600, T: 200, Avg. loss: 2634.084615\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.68, NNZs: 3, Bias: -0.001100, T: 400, Avg. loss: 2593.136418\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.76, NNZs: 3, Bias: -0.000900, T: 600, Avg. loss: 3308.216351\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.77, NNZs: 3, Bias: -0.002700, T: 800, Avg. loss: 3155.085896\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.93, NNZs: 3, Bias: -0.002800, T: 1000, Avg. loss: 3080.501847\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.43, NNZs: 3, Bias: -0.002700, T: 1200, Avg. loss: 3011.887174\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.69, NNZs: 3, Bias: -0.002200, T: 1400, Avg. loss: 3002.132514\n",
            "Total training time: 0.00 seconds.\n",
            "Convergence after 7 epochs took 0.00 seconds\n",
            "[[ 0.38249139 -0.55764501  0.15407861]]\n",
            "[-0.0022]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2YRBSC3m4LM",
        "outputId": "3ee04618-63d9-4b33-9c8e-947d16ff2e11"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_std = scaler.fit_transform(X)\n",
        "clf_LR_std = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
        "clf_LR_std.fit(X_std,Y)\n",
        "print(clf_LR_std.coef_)\n",
        "print(clf_LR_std.intercept_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 0.01, NNZs: 3, Bias: 0.000001, T: 200, Avg. loss: 0.691431\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.02, NNZs: 3, Bias: 0.000002, T: 400, Avg. loss: 0.687922\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.03, NNZs: 3, Bias: 0.000002, T: 600, Avg. loss: 0.684449\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.03, NNZs: 3, Bias: 0.000003, T: 800, Avg. loss: 0.681011\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.04, NNZs: 3, Bias: 0.000003, T: 1000, Avg. loss: 0.677608\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.05, NNZs: 3, Bias: 0.000003, T: 1200, Avg. loss: 0.674240\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.06, NNZs: 3, Bias: 0.000003, T: 1400, Avg. loss: 0.670905\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.07, NNZs: 3, Bias: 0.000003, T: 1600, Avg. loss: 0.667605\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.07, NNZs: 3, Bias: 0.000002, T: 1800, Avg. loss: 0.664338\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.08, NNZs: 3, Bias: 0.000002, T: 2000, Avg. loss: 0.661104\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.09, NNZs: 3, Bias: 0.000002, T: 2200, Avg. loss: 0.657903\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.10, NNZs: 3, Bias: 0.000001, T: 2400, Avg. loss: 0.654734\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.11, NNZs: 3, Bias: 0.000001, T: 2600, Avg. loss: 0.651598\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 0.11, NNZs: 3, Bias: 0.000001, T: 2800, Avg. loss: 0.648493\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 0.12, NNZs: 3, Bias: 0.000001, T: 3000, Avg. loss: 0.645419\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 0.13, NNZs: 3, Bias: 0.000002, T: 3200, Avg. loss: 0.642377\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 0.14, NNZs: 3, Bias: 0.000002, T: 3400, Avg. loss: 0.639365\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 0.14, NNZs: 3, Bias: 0.000001, T: 3600, Avg. loss: 0.636383\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 0.15, NNZs: 3, Bias: 0.000001, T: 3800, Avg. loss: 0.633431\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 0.16, NNZs: 3, Bias: 0.000001, T: 4000, Avg. loss: 0.630509\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 0.17, NNZs: 3, Bias: 0.000001, T: 4200, Avg. loss: 0.627616\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 0.18, NNZs: 3, Bias: 0.000001, T: 4400, Avg. loss: 0.624752\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 0.18, NNZs: 3, Bias: 0.000001, T: 4600, Avg. loss: 0.621917\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 0.19, NNZs: 3, Bias: 0.000000, T: 4800, Avg. loss: 0.619110\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 0.20, NNZs: 3, Bias: -0.000000, T: 5000, Avg. loss: 0.616331\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 0.21, NNZs: 3, Bias: -0.000001, T: 5200, Avg. loss: 0.613579\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 0.21, NNZs: 3, Bias: -0.000001, T: 5400, Avg. loss: 0.610855\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 0.22, NNZs: 3, Bias: -0.000001, T: 5600, Avg. loss: 0.608158\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 0.23, NNZs: 3, Bias: -0.000002, T: 5800, Avg. loss: 0.605488\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 0.23, NNZs: 3, Bias: -0.000003, T: 6000, Avg. loss: 0.602845\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 0.24, NNZs: 3, Bias: -0.000002, T: 6200, Avg. loss: 0.600227\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 0.25, NNZs: 3, Bias: -0.000002, T: 6400, Avg. loss: 0.597635\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 0.26, NNZs: 3, Bias: -0.000003, T: 6600, Avg. loss: 0.595069\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 0.26, NNZs: 3, Bias: -0.000003, T: 6800, Avg. loss: 0.592528\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 0.27, NNZs: 3, Bias: -0.000003, T: 7000, Avg. loss: 0.590011\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 0.28, NNZs: 3, Bias: -0.000004, T: 7200, Avg. loss: 0.587519\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 0.28, NNZs: 3, Bias: -0.000004, T: 7400, Avg. loss: 0.585052\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 0.29, NNZs: 3, Bias: -0.000004, T: 7600, Avg. loss: 0.582608\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 0.30, NNZs: 3, Bias: -0.000005, T: 7800, Avg. loss: 0.580189\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 0.30, NNZs: 3, Bias: -0.000005, T: 8000, Avg. loss: 0.577793\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 0.31, NNZs: 3, Bias: -0.000007, T: 8200, Avg. loss: 0.575420\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 0.32, NNZs: 3, Bias: -0.000008, T: 8400, Avg. loss: 0.573071\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 0.33, NNZs: 3, Bias: -0.000009, T: 8600, Avg. loss: 0.570743\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 0.33, NNZs: 3, Bias: -0.000010, T: 8800, Avg. loss: 0.568439\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 0.34, NNZs: 3, Bias: -0.000010, T: 9000, Avg. loss: 0.566156\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 0.35, NNZs: 3, Bias: -0.000010, T: 9200, Avg. loss: 0.563896\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 0.35, NNZs: 3, Bias: -0.000012, T: 9400, Avg. loss: 0.561657\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 0.36, NNZs: 3, Bias: -0.000012, T: 9600, Avg. loss: 0.559439\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 0.37, NNZs: 3, Bias: -0.000013, T: 9800, Avg. loss: 0.557243\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 0.37, NNZs: 3, Bias: -0.000015, T: 10000, Avg. loss: 0.555067\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 0.38, NNZs: 3, Bias: -0.000016, T: 10200, Avg. loss: 0.552912\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 0.39, NNZs: 3, Bias: -0.000017, T: 10400, Avg. loss: 0.550777\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 0.39, NNZs: 3, Bias: -0.000018, T: 10600, Avg. loss: 0.548663\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 0.40, NNZs: 3, Bias: -0.000020, T: 10800, Avg. loss: 0.546568\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 0.40, NNZs: 3, Bias: -0.000021, T: 11000, Avg. loss: 0.544493\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 0.41, NNZs: 3, Bias: -0.000023, T: 11200, Avg. loss: 0.542438\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 0.42, NNZs: 3, Bias: -0.000024, T: 11400, Avg. loss: 0.540402\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 0.42, NNZs: 3, Bias: -0.000025, T: 11600, Avg. loss: 0.538384\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 0.43, NNZs: 3, Bias: -0.000027, T: 11800, Avg. loss: 0.536386\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 0.44, NNZs: 3, Bias: -0.000028, T: 12000, Avg. loss: 0.534406\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 0.44, NNZs: 3, Bias: -0.000030, T: 12200, Avg. loss: 0.532444\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 0.45, NNZs: 3, Bias: -0.000030, T: 12400, Avg. loss: 0.530501\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 0.45, NNZs: 3, Bias: -0.000032, T: 12600, Avg. loss: 0.528575\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 0.46, NNZs: 3, Bias: -0.000034, T: 12800, Avg. loss: 0.526668\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 0.47, NNZs: 3, Bias: -0.000035, T: 13000, Avg. loss: 0.524777\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 0.47, NNZs: 3, Bias: -0.000038, T: 13200, Avg. loss: 0.522904\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 0.48, NNZs: 3, Bias: -0.000039, T: 13400, Avg. loss: 0.521048\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 0.49, NNZs: 3, Bias: -0.000042, T: 13600, Avg. loss: 0.519209\n",
            "Total training time: 0.11 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 0.49, NNZs: 3, Bias: -0.000046, T: 13800, Avg. loss: 0.517387\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 0.50, NNZs: 3, Bias: -0.000049, T: 14000, Avg. loss: 0.515581\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 0.50, NNZs: 3, Bias: -0.000054, T: 14200, Avg. loss: 0.513791\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 0.51, NNZs: 3, Bias: -0.000057, T: 14400, Avg. loss: 0.512018\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 0.52, NNZs: 3, Bias: -0.000060, T: 14600, Avg. loss: 0.510260\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 0.52, NNZs: 3, Bias: -0.000064, T: 14800, Avg. loss: 0.508518\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 0.53, NNZs: 3, Bias: -0.000067, T: 15000, Avg. loss: 0.506792\n",
            "Total training time: 0.12 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 0.53, NNZs: 3, Bias: -0.000069, T: 15200, Avg. loss: 0.505081\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 0.54, NNZs: 3, Bias: -0.000072, T: 15400, Avg. loss: 0.503385\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 0.54, NNZs: 3, Bias: -0.000076, T: 15600, Avg. loss: 0.501705\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 0.55, NNZs: 3, Bias: -0.000080, T: 15800, Avg. loss: 0.500039\n",
            "Total training time: 0.13 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 0.56, NNZs: 3, Bias: -0.000084, T: 16000, Avg. loss: 0.498387\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 0.56, NNZs: 3, Bias: -0.000088, T: 16200, Avg. loss: 0.496751\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 0.57, NNZs: 3, Bias: -0.000091, T: 16400, Avg. loss: 0.495128\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 0.57, NNZs: 3, Bias: -0.000095, T: 16600, Avg. loss: 0.493520\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 0.58, NNZs: 3, Bias: -0.000100, T: 16800, Avg. loss: 0.491925\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 0.58, NNZs: 3, Bias: -0.000104, T: 17000, Avg. loss: 0.490345\n",
            "Total training time: 0.14 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 0.59, NNZs: 3, Bias: -0.000108, T: 17200, Avg. loss: 0.488778\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 0.60, NNZs: 3, Bias: -0.000113, T: 17400, Avg. loss: 0.487225\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 0.60, NNZs: 3, Bias: -0.000118, T: 17600, Avg. loss: 0.485685\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 0.61, NNZs: 3, Bias: -0.000123, T: 17800, Avg. loss: 0.484158\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 0.61, NNZs: 3, Bias: -0.000128, T: 18000, Avg. loss: 0.482644\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 0.62, NNZs: 3, Bias: -0.000132, T: 18200, Avg. loss: 0.481144\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 0.62, NNZs: 3, Bias: -0.000137, T: 18400, Avg. loss: 0.479656\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 0.63, NNZs: 3, Bias: -0.000143, T: 18600, Avg. loss: 0.478180\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 0.63, NNZs: 3, Bias: -0.000147, T: 18800, Avg. loss: 0.476718\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 0.64, NNZs: 3, Bias: -0.000153, T: 19000, Avg. loss: 0.475267\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 0.64, NNZs: 3, Bias: -0.000158, T: 19200, Avg. loss: 0.473829\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 0.65, NNZs: 3, Bias: -0.000164, T: 19400, Avg. loss: 0.472402\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 0.65, NNZs: 3, Bias: -0.000170, T: 19600, Avg. loss: 0.470988\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 0.66, NNZs: 3, Bias: -0.000176, T: 19800, Avg. loss: 0.469585\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 0.67, NNZs: 3, Bias: -0.000182, T: 20000, Avg. loss: 0.468194\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 0.67, NNZs: 3, Bias: -0.000188, T: 20200, Avg. loss: 0.466815\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 0.68, NNZs: 3, Bias: -0.000194, T: 20400, Avg. loss: 0.465447\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 0.68, NNZs: 3, Bias: -0.000201, T: 20600, Avg. loss: 0.464090\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 0.69, NNZs: 3, Bias: -0.000207, T: 20800, Avg. loss: 0.462744\n",
            "Total training time: 0.20 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 0.69, NNZs: 3, Bias: -0.000215, T: 21000, Avg. loss: 0.461410\n",
            "Total training time: 0.22 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 0.70, NNZs: 3, Bias: -0.000222, T: 21200, Avg. loss: 0.460086\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 0.70, NNZs: 3, Bias: -0.000228, T: 21400, Avg. loss: 0.458773\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 0.71, NNZs: 3, Bias: -0.000235, T: 21600, Avg. loss: 0.457471\n",
            "Total training time: 0.23 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 0.71, NNZs: 3, Bias: -0.000241, T: 21800, Avg. loss: 0.456179\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 0.72, NNZs: 3, Bias: -0.000249, T: 22000, Avg. loss: 0.454898\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 0.72, NNZs: 3, Bias: -0.000255, T: 22200, Avg. loss: 0.453627\n",
            "Total training time: 0.24 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 0.73, NNZs: 3, Bias: -0.000262, T: 22400, Avg. loss: 0.452366\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 0.73, NNZs: 3, Bias: -0.000269, T: 22600, Avg. loss: 0.451115\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 0.74, NNZs: 3, Bias: -0.000277, T: 22800, Avg. loss: 0.449874\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 0.74, NNZs: 3, Bias: -0.000284, T: 23000, Avg. loss: 0.448643\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 0.75, NNZs: 3, Bias: -0.000292, T: 23200, Avg. loss: 0.447422\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 0.75, NNZs: 3, Bias: -0.000300, T: 23400, Avg. loss: 0.446210\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 0.76, NNZs: 3, Bias: -0.000308, T: 23600, Avg. loss: 0.445008\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 0.76, NNZs: 3, Bias: -0.000316, T: 23800, Avg. loss: 0.443816\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 0.77, NNZs: 3, Bias: -0.000326, T: 24000, Avg. loss: 0.442632\n",
            "Total training time: 0.25 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 0.77, NNZs: 3, Bias: -0.000334, T: 24200, Avg. loss: 0.441458\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 0.78, NNZs: 3, Bias: -0.000343, T: 24400, Avg. loss: 0.440293\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 0.78, NNZs: 3, Bias: -0.000352, T: 24600, Avg. loss: 0.439138\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 0.79, NNZs: 3, Bias: -0.000362, T: 24800, Avg. loss: 0.437991\n",
            "Total training time: 0.26 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 0.79, NNZs: 3, Bias: -0.000370, T: 25000, Avg. loss: 0.436852\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 0.79, NNZs: 3, Bias: -0.000378, T: 25200, Avg. loss: 0.435723\n",
            "Total training time: 0.27 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 0.80, NNZs: 3, Bias: -0.000388, T: 25400, Avg. loss: 0.434602\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 0.80, NNZs: 3, Bias: -0.000397, T: 25600, Avg. loss: 0.433490\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 0.81, NNZs: 3, Bias: -0.000407, T: 25800, Avg. loss: 0.432387\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 0.81, NNZs: 3, Bias: -0.000417, T: 26000, Avg. loss: 0.431291\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 0.82, NNZs: 3, Bias: -0.000426, T: 26200, Avg. loss: 0.430204\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 0.82, NNZs: 3, Bias: -0.000436, T: 26400, Avg. loss: 0.429125\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 133\n",
            "Norm: 0.83, NNZs: 3, Bias: -0.000446, T: 26600, Avg. loss: 0.428055\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 134\n",
            "Norm: 0.83, NNZs: 3, Bias: -0.000457, T: 26800, Avg. loss: 0.426992\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 135\n",
            "Norm: 0.84, NNZs: 3, Bias: -0.000467, T: 27000, Avg. loss: 0.425937\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 136\n",
            "Norm: 0.84, NNZs: 3, Bias: -0.000478, T: 27200, Avg. loss: 0.424891\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 137\n",
            "Norm: 0.85, NNZs: 3, Bias: -0.000490, T: 27400, Avg. loss: 0.423851\n",
            "Total training time: 0.32 seconds.\n",
            "-- Epoch 138\n",
            "Norm: 0.85, NNZs: 3, Bias: -0.000501, T: 27600, Avg. loss: 0.422820\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 139\n",
            "Norm: 0.85, NNZs: 3, Bias: -0.000512, T: 27800, Avg. loss: 0.421796\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 140\n",
            "Norm: 0.86, NNZs: 3, Bias: -0.000523, T: 28000, Avg. loss: 0.420780\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 141\n",
            "Norm: 0.86, NNZs: 3, Bias: -0.000534, T: 28200, Avg. loss: 0.419771\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 142\n",
            "Norm: 0.87, NNZs: 3, Bias: -0.000546, T: 28400, Avg. loss: 0.418770\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 143\n",
            "Norm: 0.87, NNZs: 3, Bias: -0.000556, T: 28600, Avg. loss: 0.417776\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 144\n",
            "Norm: 0.88, NNZs: 3, Bias: -0.000568, T: 28800, Avg. loss: 0.416789\n",
            "Total training time: 0.35 seconds.\n",
            "-- Epoch 145\n",
            "Norm: 0.88, NNZs: 3, Bias: -0.000579, T: 29000, Avg. loss: 0.415809\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 146\n",
            "Norm: 0.89, NNZs: 3, Bias: -0.000591, T: 29200, Avg. loss: 0.414836\n",
            "Total training time: 0.36 seconds.\n",
            "-- Epoch 147\n",
            "Norm: 0.89, NNZs: 3, Bias: -0.000603, T: 29400, Avg. loss: 0.413871\n",
            "Total training time: 0.37 seconds.\n",
            "Convergence after 147 epochs took 0.37 seconds\n",
            "[[ 0.03851242 -0.00553122  0.88963322]]\n",
            "[-0.00060297]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqgkHP92nhtC",
        "outputId": "5aa7c56b-23ea-40ee-c412-3cad681a716d"
      },
      "source": [
        "clf_SVM_std = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='hinge', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
        "clf_SVM_std.fit(X_std,Y)\n",
        "print(clf_SVM_std.coef_)\n",
        "print(clf_SVM_std.intercept_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 0.02, NNZs: 3, Bias: 0.000000, T: 200, Avg. loss: 0.993111\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 0.03, NNZs: 3, Bias: -0.000000, T: 400, Avg. loss: 0.978934\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 0.05, NNZs: 3, Bias: 0.000000, T: 600, Avg. loss: 0.964757\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 800, Avg. loss: 0.950580\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 0.08, NNZs: 3, Bias: -0.000000, T: 1000, Avg. loss: 0.936403\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 0.10, NNZs: 3, Bias: 0.000000, T: 1200, Avg. loss: 0.922226\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 0.12, NNZs: 3, Bias: 0.000000, T: 1400, Avg. loss: 0.908049\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 0.13, NNZs: 3, Bias: 0.000000, T: 1600, Avg. loss: 0.893873\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 0.15, NNZs: 3, Bias: 0.000000, T: 1800, Avg. loss: 0.879696\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 0.17, NNZs: 3, Bias: 0.000000, T: 2000, Avg. loss: 0.865519\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 0.19, NNZs: 3, Bias: -0.000000, T: 2200, Avg. loss: 0.851342\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 0.20, NNZs: 3, Bias: -0.000000, T: 2400, Avg. loss: 0.837165\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 0.22, NNZs: 3, Bias: -0.000000, T: 2600, Avg. loss: 0.822988\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 0.24, NNZs: 3, Bias: -0.000000, T: 2800, Avg. loss: 0.808812\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 0.25, NNZs: 3, Bias: 0.000000, T: 3000, Avg. loss: 0.794635\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 0.27, NNZs: 3, Bias: 0.000000, T: 3200, Avg. loss: 0.780458\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 0.29, NNZs: 3, Bias: -0.000000, T: 3400, Avg. loss: 0.766282\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 0.30, NNZs: 3, Bias: 0.000000, T: 3600, Avg. loss: 0.752105\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 0.32, NNZs: 3, Bias: 0.000000, T: 3800, Avg. loss: 0.737929\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 0.34, NNZs: 3, Bias: -0.000000, T: 4000, Avg. loss: 0.723752\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 0.35, NNZs: 3, Bias: 0.000000, T: 4200, Avg. loss: 0.709575\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 0.37, NNZs: 3, Bias: 0.000000, T: 4400, Avg. loss: 0.695399\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 0.39, NNZs: 3, Bias: -0.000000, T: 4600, Avg. loss: 0.681222\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 0.40, NNZs: 3, Bias: 0.000000, T: 4800, Avg. loss: 0.667046\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 0.42, NNZs: 3, Bias: 0.000000, T: 5000, Avg. loss: 0.652870\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 0.44, NNZs: 3, Bias: 0.000000, T: 5200, Avg. loss: 0.638693\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 0.45, NNZs: 3, Bias: 0.000000, T: 5400, Avg. loss: 0.624517\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 0.47, NNZs: 3, Bias: -0.000000, T: 5600, Avg. loss: 0.610341\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 0.49, NNZs: 3, Bias: -0.000000, T: 5800, Avg. loss: 0.596164\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 0.51, NNZs: 3, Bias: -0.000000, T: 6000, Avg. loss: 0.581988\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 0.52, NNZs: 3, Bias: -0.000000, T: 6200, Avg. loss: 0.567812\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 0.54, NNZs: 3, Bias: -0.000000, T: 6400, Avg. loss: 0.553635\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 0.56, NNZs: 3, Bias: -0.000100, T: 6600, Avg. loss: 0.539559\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 0.57, NNZs: 3, Bias: -0.000400, T: 6800, Avg. loss: 0.525831\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 0.59, NNZs: 3, Bias: -0.000400, T: 7000, Avg. loss: 0.512822\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 0.60, NNZs: 3, Bias: -0.000300, T: 7200, Avg. loss: 0.500814\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 0.62, NNZs: 3, Bias: 0.000100, T: 7400, Avg. loss: 0.489867\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 0.63, NNZs: 3, Bias: 0.000500, T: 7600, Avg. loss: 0.479843\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 0.64, NNZs: 3, Bias: 0.000900, T: 7800, Avg. loss: 0.470023\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 0.66, NNZs: 3, Bias: 0.001200, T: 8000, Avg. loss: 0.460939\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 0.67, NNZs: 3, Bias: 0.001100, T: 8200, Avg. loss: 0.453123\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 0.68, NNZs: 3, Bias: 0.001000, T: 8400, Avg. loss: 0.446506\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 0.69, NNZs: 3, Bias: 0.001000, T: 8600, Avg. loss: 0.440244\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 0.70, NNZs: 3, Bias: 0.001000, T: 8800, Avg. loss: 0.434183\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 0.71, NNZs: 3, Bias: 0.001200, T: 9000, Avg. loss: 0.428457\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 0.72, NNZs: 3, Bias: 0.001400, T: 9200, Avg. loss: 0.422915\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 0.73, NNZs: 3, Bias: 0.001500, T: 9400, Avg. loss: 0.417423\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 0.74, NNZs: 3, Bias: 0.001800, T: 9600, Avg. loss: 0.412336\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 0.75, NNZs: 3, Bias: 0.001900, T: 9800, Avg. loss: 0.407610\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 0.76, NNZs: 3, Bias: 0.002200, T: 10000, Avg. loss: 0.403083\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 0.77, NNZs: 3, Bias: 0.002500, T: 10200, Avg. loss: 0.398796\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 0.78, NNZs: 3, Bias: 0.002900, T: 10400, Avg. loss: 0.394692\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 0.79, NNZs: 3, Bias: 0.003400, T: 10600, Avg. loss: 0.390702\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 0.80, NNZs: 3, Bias: 0.004000, T: 10800, Avg. loss: 0.386766\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 0.81, NNZs: 3, Bias: 0.004800, T: 11000, Avg. loss: 0.383045\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 0.81, NNZs: 3, Bias: 0.005600, T: 11200, Avg. loss: 0.379523\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 0.82, NNZs: 3, Bias: 0.006400, T: 11400, Avg. loss: 0.376092\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 0.83, NNZs: 3, Bias: 0.006900, T: 11600, Avg. loss: 0.372853\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 0.84, NNZs: 3, Bias: 0.007400, T: 11800, Avg. loss: 0.369757\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 0.85, NNZs: 3, Bias: 0.007900, T: 12000, Avg. loss: 0.366660\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 0.85, NNZs: 3, Bias: 0.008500, T: 12200, Avg. loss: 0.363659\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 0.86, NNZs: 3, Bias: 0.008900, T: 12400, Avg. loss: 0.360918\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 0.87, NNZs: 3, Bias: 0.009300, T: 12600, Avg. loss: 0.358293\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 0.87, NNZs: 3, Bias: 0.009700, T: 12800, Avg. loss: 0.355741\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 0.88, NNZs: 3, Bias: 0.010200, T: 13000, Avg. loss: 0.353291\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 0.89, NNZs: 3, Bias: 0.010500, T: 13200, Avg. loss: 0.350947\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 0.89, NNZs: 3, Bias: 0.010800, T: 13400, Avg. loss: 0.348714\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 0.90, NNZs: 3, Bias: 0.011000, T: 13600, Avg. loss: 0.346524\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 0.91, NNZs: 3, Bias: 0.011000, T: 13800, Avg. loss: 0.344410\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 0.91, NNZs: 3, Bias: 0.011000, T: 14000, Avg. loss: 0.342392\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 0.92, NNZs: 3, Bias: 0.010900, T: 14200, Avg. loss: 0.340372\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 0.93, NNZs: 3, Bias: 0.010800, T: 14400, Avg. loss: 0.338400\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 0.93, NNZs: 3, Bias: 0.010600, T: 14600, Avg. loss: 0.336501\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 0.94, NNZs: 3, Bias: 0.010400, T: 14800, Avg. loss: 0.334604\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 0.94, NNZs: 3, Bias: 0.010400, T: 15000, Avg. loss: 0.332782\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 0.95, NNZs: 3, Bias: 0.010300, T: 15200, Avg. loss: 0.331037\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 0.96, NNZs: 3, Bias: 0.010200, T: 15400, Avg. loss: 0.329317\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 0.96, NNZs: 3, Bias: 0.010100, T: 15600, Avg. loss: 0.327597\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 0.97, NNZs: 3, Bias: 0.009900, T: 15800, Avg. loss: 0.325932\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 0.97, NNZs: 3, Bias: 0.009900, T: 16000, Avg. loss: 0.324369\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 0.98, NNZs: 3, Bias: 0.009900, T: 16200, Avg. loss: 0.322840\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 0.98, NNZs: 3, Bias: 0.009900, T: 16400, Avg. loss: 0.321310\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 0.99, NNZs: 3, Bias: 0.010000, T: 16600, Avg. loss: 0.319872\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 0.99, NNZs: 3, Bias: 0.010100, T: 16800, Avg. loss: 0.318513\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 1.00, NNZs: 3, Bias: 0.010300, T: 17000, Avg. loss: 0.317175\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 1.00, NNZs: 3, Bias: 0.010500, T: 17200, Avg. loss: 0.315862\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 1.01, NNZs: 3, Bias: 0.010700, T: 17400, Avg. loss: 0.314549\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 1.01, NNZs: 3, Bias: 0.010900, T: 17600, Avg. loss: 0.313237\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 1.02, NNZs: 3, Bias: 0.011100, T: 17800, Avg. loss: 0.311924\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 1.02, NNZs: 3, Bias: 0.011300, T: 18000, Avg. loss: 0.310612\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 1.03, NNZs: 3, Bias: 0.011500, T: 18200, Avg. loss: 0.309337\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 1.03, NNZs: 3, Bias: 0.011700, T: 18400, Avg. loss: 0.308166\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 1.04, NNZs: 3, Bias: 0.011900, T: 18600, Avg. loss: 0.307057\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 1.04, NNZs: 3, Bias: 0.012200, T: 18800, Avg. loss: 0.305980\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 1.05, NNZs: 3, Bias: 0.012500, T: 19000, Avg. loss: 0.304914\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 1.05, NNZs: 3, Bias: 0.012800, T: 19200, Avg. loss: 0.303880\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 1.06, NNZs: 3, Bias: 0.013100, T: 19400, Avg. loss: 0.302892\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 1.06, NNZs: 3, Bias: 0.013500, T: 19600, Avg. loss: 0.301927\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 1.07, NNZs: 3, Bias: 0.013900, T: 19800, Avg. loss: 0.300979\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 1.07, NNZs: 3, Bias: 0.014100, T: 20000, Avg. loss: 0.300054\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 1.07, NNZs: 3, Bias: 0.014300, T: 20200, Avg. loss: 0.299188\n",
            "Total training time: 0.04 seconds.\n",
            "Convergence after 101 epochs took 0.04 seconds\n",
            "[[0.04248904 0.02585179 1.07272982]]\n",
            "[0.0143]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f6m0hEKn-0Q",
        "outputId": "31a78936-c00c-48aa-cb34-480adfc02156"
      },
      "source": [
        "from tabulate import tabulate\n",
        "summary_lst = []\n",
        "summary_lst.append([\"Logistic Regression\", \"No standardization\", clf_lr_nstd.coef_[0][0], clf_lr_nstd.coef_[0][1],clf_lr_nstd.coef_[0][2],clf_lr_nstd.intercept_[0]])\n",
        "summary_lst.append([\"Logistic Regression\", \"Standardization\", clf_LR_std.coef_[0][0], clf_LR_std.coef_[0][1],clf_LR_std.coef_[0][2],clf_LR_std.intercept_[0]])\n",
        "summary_lst.append([\"SVM\", \"No standardization\", clf_SVM_Nstd.coef_[0][0], clf_SVM_Nstd.coef_[0][1],clf_SVM_Nstd.coef_[0][2],clf_SVM_Nstd.intercept_[0]])\n",
        "summary_lst.append([\"SVM\", \"Standardization\", clf_SVM_std.coef_[0][0], clf_SVM_std.coef_[0][1],clf_SVM_std.coef_[0][2],clf_SVM_std.intercept_[0]])\n",
        "print(tabulate(summary_lst, headers=['Classification Type', 'Data Standardization status', 'feature1_weight_factor', 'feature2_weight_factor', 'feature3_weight_factor', 'Intercept']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Type    Data Standardization status      feature1_weight_factor    feature2_weight_factor    feature3_weight_factor     Intercept\n",
            "---------------------  -----------------------------  ------------------------  ------------------------  ------------------------  ------------\n",
            "Logistic Regression    No standardization                            0.371705                -1.34464                     0.12669   -0.00352309\n",
            "Logistic Regression    Standardization                               0.0385124               -0.00553122                  0.889633  -0.000602971\n",
            "SVM                    No standardization                            0.382491                -0.557645                    0.154079  -0.0022\n",
            "SVM                    Standardization                               0.042489                 0.0258518                   1.07273    0.0143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbMnsrxakbhi"
      },
      "source": [
        "<h3><font color='blue'> Make sure you write the observations for each task, why a particular feautre got more importance than others</font></h3>"
      ]
    }
  ]
}