{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HExLQrE4ZxR"
   },
   "source": [
    "<h1><font color='blue'> 8E and 8F: Finding the Probability P(Y==1|X)</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LuKrFzC4ZxV"
   },
   "source": [
    "<h2><font color='Geen'> 8E: Implementing Decision Function of SVM RBF Kernel</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wES-wWN4ZxX"
   },
   "source": [
    "<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n",
    "\n",
    "Check the documentation for better understanding of these attributes: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "<img src='https://i.imgur.com/K11msU4.png' width=500>\n",
    "\n",
    "As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n",
    "\n",
    "Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n",
    "\n",
    "Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n",
    "\n",
    "Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n",
    "$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n",
    "\n",
    "RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n",
    "\n",
    "For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z830CfMk4Zxa"
   },
   "source": [
    "## Task E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuBxHiCQ4Zxc"
   },
   "source": [
    "> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n",
    "\n",
    "> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n",
    "\n",
    "> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1556,
     "status": "ok",
     "timestamp": 1635594074732,
     "user": {
      "displayName": "Lakshmi Varaprasad",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2Tvorv-DI-1hswdq5XusI1T188IvZxlkPoQuOOg=s64",
      "userId": "13146185712406457021"
     },
     "user_tz": -330
    },
    "id": "fCgMNEvI4Zxf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1635594079139,
     "user": {
      "displayName": "Lakshmi Varaprasad",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2Tvorv-DI-1hswdq5XusI1T188IvZxlkPoQuOOg=s64",
      "userId": "13146185712406457021"
     },
     "user_tz": -330
    },
    "id": "ANUNIqCe4Zxn",
    "outputId": "297dbf21-eba7-478a-d98b-c175190d7cb0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3000, 5), (1000, 5), (1000, 5))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train)\n",
    "X_train.shape, X_cv.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHie1zqH4Zxt"
   },
   "source": [
    "### Pseudo code\n",
    "\n",
    "clf = SVC(gamma=0.001, C=100.)<br>\n",
    "clf.fit(Xtrain, ytrain)\n",
    "\n",
    "<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n",
    "   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n",
    "    \n",
    "fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n",
    "\n",
    "<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "output_embedded_package_id": "1jkgNRoYS8-mLUefucefVYbRJtg6sWxMU"
    },
    "id": "h43kDT3M41u5",
    "outputId": "07790462-46b9-4c89-a7d3-b17e51489d11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.74439228  1.77714849  1.64888727  1.52845213  0.55574548]\n",
      "[[-0.74439228]\n",
      " [ 1.77714849]\n",
      " [ 1.64888727]\n",
      " [ 1.52845213]\n",
      " [ 0.55574548]]\n"
     ]
    }
   ],
   "source": [
    "# you can write your code here\n",
    "from sklearn.svm import SVC\n",
    "import math\n",
    "\n",
    "def decision_function(X_cv,clf):\n",
    "  cv_desc_func = []\n",
    "  for x_q in X_cv:\n",
    "        decision_x_q = np.sum(clf.dual_coef_*np.exp(-clf._gamma*np.sum((clf.support_vectors_-x_q)**2,axis=1)))+clf.intercept_\n",
    "        cv_desc_func.append(decision_x_q)\n",
    "  return np.array(cv_desc_func)\n",
    "\n",
    "gamma_val = 0.001\n",
    "clf = SVC(C=100, kernel='rbf', gamma=gamma_val)\n",
    "clf.fit(X_train,y_train)\n",
    "decis_func = clf.decision_function(X_cv)\n",
    "print(decis_func[:5])\n",
    "decis_func_calc = decision_function(X_cv,clf)\n",
    "print(decis_func_calc[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0bKCboN4Zxu"
   },
   "source": [
    "<h2><font color='Geen'> 8F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMn7OEN94Zxw"
   },
   "source": [
    "Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n",
    "<img src='https://i.imgur.com/CAMnVnh.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0n5EFkx4Zxz"
   },
   "source": [
    "## TASK F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0HOqVJq4Zx1"
   },
   "source": [
    "\n",
    "> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n",
    "\n",
    "> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n",
    "\n",
    "> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n",
    "<img src='https://i.imgur.com/zKYE9Oc.png'>\n",
    "if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n",
    "\n",
    "> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.11, NNZs: 5, Bias: -0.055714, T: 3000, Avg. loss: 0.665312\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.22, NNZs: 5, Bias: -0.104952, T: 6000, Avg. loss: 0.615582\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.32, NNZs: 5, Bias: -0.148501, T: 9000, Avg. loss: 0.573666\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.41, NNZs: 5, Bias: -0.187073, T: 12000, Avg. loss: 0.538114\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.50, NNZs: 5, Bias: -0.221359, T: 15000, Avg. loss: 0.507724\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.58, NNZs: 5, Bias: -0.251881, T: 18000, Avg. loss: 0.481548\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.65, NNZs: 5, Bias: -0.279206, T: 21000, Avg. loss: 0.458816\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.72, NNZs: 5, Bias: -0.303773, T: 24000, Avg. loss: 0.438922\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.79, NNZs: 5, Bias: -0.325819, T: 27000, Avg. loss: 0.421393\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.85, NNZs: 5, Bias: -0.345821, T: 30000, Avg. loss: 0.405847\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.91, NNZs: 5, Bias: -0.363962, T: 33000, Avg. loss: 0.391967\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.97, NNZs: 5, Bias: -0.380484, T: 36000, Avg. loss: 0.379513\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.02, NNZs: 5, Bias: -0.395566, T: 39000, Avg. loss: 0.368277\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.07, NNZs: 5, Bias: -0.409397, T: 42000, Avg. loss: 0.358089\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.12, NNZs: 5, Bias: -0.422125, T: 45000, Avg. loss: 0.348810\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1.17, NNZs: 5, Bias: -0.433841, T: 48000, Avg. loss: 0.340329\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.22, NNZs: 5, Bias: -0.444723, T: 51000, Avg. loss: 0.332548\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1.26, NNZs: 5, Bias: -0.454820, T: 54000, Avg. loss: 0.325381\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1.30, NNZs: 5, Bias: -0.464229, T: 57000, Avg. loss: 0.318762\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1.35, NNZs: 5, Bias: -0.473038, T: 60000, Avg. loss: 0.312631\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1.39, NNZs: 5, Bias: -0.481255, T: 63000, Avg. loss: 0.306936\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1.42, NNZs: 5, Bias: -0.489007, T: 66000, Avg. loss: 0.301633\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1.46, NNZs: 5, Bias: -0.496288, T: 69000, Avg. loss: 0.296685\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1.50, NNZs: 5, Bias: -0.503169, T: 72000, Avg. loss: 0.292057\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1.53, NNZs: 5, Bias: -0.509642, T: 75000, Avg. loss: 0.287720\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1.57, NNZs: 5, Bias: -0.515786, T: 78000, Avg. loss: 0.283649\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.60, NNZs: 5, Bias: -0.521626, T: 81000, Avg. loss: 0.279819\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1.63, NNZs: 5, Bias: -0.527179, T: 84000, Avg. loss: 0.276212\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.66, NNZs: 5, Bias: -0.532470, T: 87000, Avg. loss: 0.272808\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.69, NNZs: 5, Bias: -0.537513, T: 90000, Avg. loss: 0.269590\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1.72, NNZs: 5, Bias: -0.542349, T: 93000, Avg. loss: 0.266546\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1.75, NNZs: 5, Bias: -0.546994, T: 96000, Avg. loss: 0.263661\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1.78, NNZs: 5, Bias: -0.551446, T: 99000, Avg. loss: 0.260924\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1.80, NNZs: 5, Bias: -0.555744, T: 102000, Avg. loss: 0.258324\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1.83, NNZs: 5, Bias: -0.559853, T: 105000, Avg. loss: 0.255852\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1.86, NNZs: 5, Bias: -0.563863, T: 108000, Avg. loss: 0.253498\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1.88, NNZs: 5, Bias: -0.567699, T: 111000, Avg. loss: 0.251255\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1.91, NNZs: 5, Bias: -0.571428, T: 114000, Avg. loss: 0.249115\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 1.93, NNZs: 5, Bias: -0.575031, T: 117000, Avg. loss: 0.247072\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1.95, NNZs: 5, Bias: -0.578503, T: 120000, Avg. loss: 0.245120\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 1.98, NNZs: 5, Bias: -0.581907, T: 123000, Avg. loss: 0.243252\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 2.00, NNZs: 5, Bias: -0.585215, T: 126000, Avg. loss: 0.241464\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 2.02, NNZs: 5, Bias: -0.588409, T: 129000, Avg. loss: 0.239751\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 2.04, NNZs: 5, Bias: -0.591518, T: 132000, Avg. loss: 0.238109\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 2.07, NNZs: 5, Bias: -0.594576, T: 135000, Avg. loss: 0.236534\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 2.09, NNZs: 5, Bias: -0.597565, T: 138000, Avg. loss: 0.235021\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 2.11, NNZs: 5, Bias: -0.600461, T: 141000, Avg. loss: 0.233568\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 2.13, NNZs: 5, Bias: -0.603298, T: 144000, Avg. loss: 0.232170\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 2.15, NNZs: 5, Bias: -0.606080, T: 147000, Avg. loss: 0.230826\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 2.17, NNZs: 5, Bias: -0.608790, T: 150000, Avg. loss: 0.229531\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 2.18, NNZs: 5, Bias: -0.611440, T: 153000, Avg. loss: 0.228285\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 2.20, NNZs: 5, Bias: -0.614060, T: 156000, Avg. loss: 0.227083\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 2.22, NNZs: 5, Bias: -0.616628, T: 159000, Avg. loss: 0.225925\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 2.24, NNZs: 5, Bias: -0.619158, T: 162000, Avg. loss: 0.224807\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 2.26, NNZs: 5, Bias: -0.621612, T: 165000, Avg. loss: 0.223728\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 2.27, NNZs: 5, Bias: -0.624033, T: 168000, Avg. loss: 0.222685\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 2.29, NNZs: 5, Bias: -0.626433, T: 171000, Avg. loss: 0.221678\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 2.31, NNZs: 5, Bias: -0.628786, T: 174000, Avg. loss: 0.220704\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 2.32, NNZs: 5, Bias: -0.631095, T: 177000, Avg. loss: 0.219763\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 2.34, NNZs: 5, Bias: -0.633382, T: 180000, Avg. loss: 0.218852\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 2.36, NNZs: 5, Bias: -0.635641, T: 183000, Avg. loss: 0.217970\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 2.37, NNZs: 5, Bias: -0.637859, T: 186000, Avg. loss: 0.217116\n",
      "Total training time: 0.05 seconds.\n",
      "Convergence after 62 epochs took 0.05 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/14 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-152cbd4dac6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_cv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_cv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_cv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meta0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-152cbd4dac6d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X_train, y_train, X_test, y_test, epochs, alpha, eta0)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mdw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_dw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_db\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0meta0\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-152cbd4dac6d>\u001b[0m in \u001b[0;36mgradient_dw\u001b[1;34m(x, y, w, b, alpha, N)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgradient_dw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;34m'''In this function, we will compute the gardient w.r.to w '''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mdw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def initialize_weights(dim):\n",
    "    ''' In this function, we will initialize our weights and bias'''\n",
    "    #initialize the weights to zeros array of (1,dim) dimensions\n",
    "    #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
    "    #initialize bias to zero\n",
    "\n",
    "    w = np.zeros_like(dim)\n",
    "    b = 0\n",
    "    \n",
    "    return w,b\n",
    "\n",
    "def sigmoid(z):\n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "    # compute sigmoid(z) and return\n",
    "    sigmoid_z = 1/(1+math.exp(-1*z))\n",
    "    return sigmoid_z\n",
    "\n",
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    '''In this function, we will compute the gardient w.r.to w '''\n",
    "    dw = x*(y-sigmoid(np.matmul(w.transpose(),x)+b))-(alpha/N)*w\n",
    "    return dw\n",
    "\n",
    "def gradient_db(x,y,w,b):\n",
    "     '''In this function, we will compute gradient w.r.to b '''\n",
    "     db = y-sigmoid(np.matmul(w.transpose(),x)+b) \n",
    "     return db\n",
    "\n",
    "def pred(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z=np.dot(w,X[i])+b\n",
    "        if sigmoid(z) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
    "            predict.append(1)\n",
    "        else:\n",
    "            predict.append(0)\n",
    "    return np.array(predict)\n",
    "\n",
    "def logloss(y_true,y_pred):\n",
    "    '''In this function, we will compute log loss '''\n",
    "    temp = 0\n",
    "    for i in range(len(y_true)):\n",
    "      y_pred_float = y_pred.astype(np.float)\n",
    "      if y_pred_float[i]<=0.0 or y_pred_float[i]==1.0:\n",
    "        if y_pred_float[i]==0.0:\n",
    "          y_pred_float[i] = 1e-12\n",
    "        elif y_pred_float[i]==1.0:\n",
    "          y_pred_float[i] = 1-1e-12\n",
    "        else:\n",
    "          y_pred_float[i] = abs(y_pred_float[i])\n",
    "      temp = temp+((y_true[i]*math.log10(y_pred_float[i]))+((1-y_true[i])*math.log10(1-y_pred_float[i])))\n",
    "    loss = -1*(temp/len(y_true))\n",
    "    return loss\n",
    "\n",
    "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
    "    ''' In this function, we will implement logistic regression'''\n",
    "    #Here eta0 is learning rate\n",
    "    #implement the code as follows\n",
    "    # initalize the weights (call the initialize_weights(X_train[0]) function)\n",
    "    # for every epoch\n",
    "        # for every data point(X_train,y_train)\n",
    "           #compute gradient w.r.to w (call the gradient_dw() function)\n",
    "           #compute gradient w.r.to b (call the gradient_db() function)\n",
    "           #update w, b\n",
    "        # predict the output of x_train[for all data points in X_train] using w,b\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        # store all the train loss values in a list\n",
    "        # predict the output of x_test[for all data points in X_test] using w,b\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        # store all the test loss values in a list\n",
    "        # you can also compare previous loss and current loss, if loss is not updating then stop the process and return w,b\n",
    "    w,b=initialize_weights(X_train[0])\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    for j in tqdm(range(epochs)):\n",
    "      for i in range(len(X_train)):\n",
    "        dw = gradient_dw(X_train[i],y_train[i],w,b,alpha,len(X_train))\n",
    "        db = gradient_db(X_train[i],y_train[i],w,b)\n",
    "        w = w+eta0*dw\n",
    "        b = b+eta0*db\n",
    "      y_true_train = y_train\n",
    "      y_pred_train = pred(w,b, X_train)  \n",
    "      loss = logloss(y_true_train,y_pred_train)\n",
    "      train_loss.append(loss)\n",
    "      y_true_test = y_test\n",
    "      y_pred_test = pred(w,b, X_test)  \n",
    "      loss = logloss(y_true_test,y_pred_test)\n",
    "      test_loss.append(loss)\n",
    "    plt.plot(list(range(epochs)),train_loss,label=\"Train dataset loss\") \n",
    "    plt.plot(list(range(epochs)),test_loss,label=\"Test dataset loss\") \n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return w,b\n",
    "\n",
    "num_pos=0\n",
    "num_neg=0\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i]>0:\n",
    "        num_pos+=1\n",
    "    else:\n",
    "        num_neg+=1\n",
    "y_pos = (num_pos+1)/(num_pos+2)\n",
    "y_neg = 1/(num_neg+2)\n",
    "y_cv = np.where(y_cv==1,y_pos,y_neg)\n",
    "\n",
    "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf.fit(X_train, y_train)\n",
    "f_cv = clf.decision_function(X_cv)\n",
    "f_test = clf.decision_function(X_test)\n",
    "\n",
    "alpha=0.0001\n",
    "eta0=0.0001\n",
    "N=len(f_cv)\n",
    "epochs=14\n",
    "w,b=train(f_cv,y_cv,f_test,y_test,epochs,alpha,eta0)\n",
    "print(w)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTY7z2bd4Zx2"
   },
   "source": [
    "__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are excluding the hyerparameter tuning part, but intrested students can try that__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CM3odN1Z4Zx3"
   },
   "source": [
    "\n",
    "If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n",
    "\n",
    "1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n",
    "\n",
    "2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n",
    "\n",
    "3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n",
    "\n",
    "4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "8E_F_LR_SVM.ipynb",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
